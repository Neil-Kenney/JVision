import cv2
import mediapipe as mp
import numpy as np
from collections import deque
import tensorflow as tf
import time
import pyautogui

SEQ_LENGTH = 20
FEATURE_SIZE = 63
CONF_THRESHOLD = 0.95
COOLDOWN_TIME = 3
FREEZE_TIME = 0.6

VELOCITY_GATE = 0.001
MIN_TRAVEL = 0.005
DEAD_ZONE = 0.005
MIN_DIR_FRAMES = 1

last_action_time = 0
freeze_until = 0
reentry_freeze_until = 0

prev_x = None
motion_acc = 0.0
dir_consistency = 0
current_dir = 0
last_block_reason = ""

model = tf.keras.models.load_model("gesture_model.h5")
label_classes = np.load("gesture_labels.npy", allow_pickle=True)

def extract_features(hand_landmarks):
    return [coord for lm in hand_landmarks.landmark for coord in (lm.x, lm.y, lm.z)]

mp_hands = mp.solutions.hands
mp_draw = mp.solutions.drawing_utils

cap = cv2.VideoCapture(0)
sequence = deque(maxlen=SEQ_LENGTH)

with mp_hands.Hands(max_num_hands=2,
                    min_detection_confidence=0.3,
                    min_tracking_confidence=0.3) as hands:

    while True:
        success, frame = cap.read()
        if not success:
            continue

        frame = cv2.resize(frame, (1280, 720))
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(rgb)

        if time.time() < freeze_until:
            cv2.putText(frame, "Freeze...", (10, 150),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 100, 255), 2)
            cv2.imshow("Gesture Control", frame)
            if cv2.waitKey(1) & 0xFF == 27: break
            continue

        if time.time() < reentry_freeze_until:
            cv2.putText(frame, "Stabilizing...", (10, 180),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 150, 0), 2)
            prev_x = None
            motion_acc = 0.0
            dir_consistency = 0
            current_dir = 0
            last_block_reason = "reentry"
            cv2.imshow("Gesture Control", frame)
            if cv2.waitKey(1) & 0xFF == 27: break
            continue

        if results.multi_hand_landmarks and len(results.multi_hand_landmarks) > 1:
            cv2.putText(frame, "Two hands detected - disabled", (10, 150),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

            prev_x = None
            motion_acc = 0.0
            dir_consistency = 0
            current_dir = 0
            last_block_reason = "two_hands"

            reentry_freeze_until = time.time() + 0.4

            cv2.imshow("Gesture Control", frame)
            if cv2.waitKey(1) & 0xFF == 27: break
            continue

        dx_debug = 0.0

        if results.multi_hand_landmarks:
            hand = results.multi_hand_landmarks[0]
            mp_draw.draw_landmarks(frame, hand, mp_hands.HAND_CONNECTIONS)

            features = extract_features(hand)
            sequence.append(features)

            tip = hand.landmark[8]
            x = tip.x

            if prev_x is not None:
                dx = x - prev_x
                dx_debug = dx

                if abs(dx) < VELOCITY_GATE:
                    motion_acc = 0.0
                    dir_consistency = 0
                    last_block_reason = "velocity"
                    prev_x = x
                else:
                    inst_dir = 1 if dx > 0 else -1
                    if inst_dir != current_dir:
                        motion_acc = 0.0
                        dir_consistency = 0
                        current_dir = inst_dir
                    else:
                        dir_consistency += 1

                    motion_acc += dx
                    last_block_reason = ""

            prev_x = x

            if 0.5 - DEAD_ZONE < x < 0.5 + DEAD_ZONE:
                last_block_reason = "dead_zone"
            elif dir_consistency < MIN_DIR_FRAMES:
                last_block_reason = "dir_frames"
            elif abs(motion_acc) < MIN_TRAVEL:
                last_block_reason = "travel"
            else:
                last_block_reason = ""

                if len(sequence) == SEQ_LENGTH:
                    seq_array = np.array(sequence, dtype=np.float32).reshape(1, SEQ_LENGTH, FEATURE_SIZE)
                    preds = model.predict(seq_array, verbose=0)[0]

                    gesture_indices = [i for i, lbl in enumerate(label_classes) if lbl != "no_gesture"]
                    best_idx = gesture_indices[np.argmax(preds[gesture_indices])]

                    pred_label = label_classes[best_idx]
                    max_conf = preds[best_idx]

                    if max_conf > CONF_THRESHOLD:
                        now = time.time()
                        if now - last_action_time > COOLDOWN_TIME:

                            if pred_label == "swipe_left":
                                print("Previous Slide")
                                pyautogui.press("left")

                            elif pred_label == "swipe_right":
                                print("Next Slide")
                                pyautogui.press("right")

                            last_action_time = now
                            freeze_until = time.time() + FREEZE_TIME

                            motion_acc = 0.0
                            prev_x = None
                            dir_consistency = 0
                            current_dir = 0

        else:
            prev_x = None
            motion_acc = 0.0
            dir_consistency = 0
            current_dir = 0
            last_block_reason = "no_hand"

        cv2.putText(frame, f"dx={dx_debug:.3f} acc={motion_acc:.3f} dir={dir_consistency}",
                    (10, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,0), 2)
        cv2.putText(frame, f"block={last_block_reason}",
                    (10, 230), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)

        cv2.imshow("Gesture Control", frame)
        if cv2.waitKey(1) & 0xFF == 27: break

cap.release()
cv2.destroyAllWindows()
